{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Strategies tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.lunarlander import LunarLanderWrapper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "1\n",
      "beta Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# test layernorm:\n",
    "lnorm = LayerNorm(4)\n",
    "for name, param in lnorm.named_parameters():\n",
    "    print(name, param)\n",
    "    print(len(param.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self, conf={}, init=True):\n",
    "        if not init:\n",
    "            return\n",
    "        \n",
    "        use_cuda = False\n",
    "        if hasattr(conf, 'disable_cuda'):\n",
    "            use_cuda = not conf.disable_cuda and torch.cuda.is_available()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.hidden_size = 6\n",
    "        self.actor_num_layers = 2\n",
    "        self.activation_layer = 'tanh'\n",
    "        \n",
    "        self.state_dim = 3\n",
    "        self.action_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'leakyRelu': nn.LeakyReLU(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test list extension:\n",
    "lst = []\n",
    "lst.extend([[1, 2, 3], [4, 5, 6]])\n",
    "[*lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test reshape:\n",
    "s = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "s.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, args: Parameters, init=False):\n",
    "        super(Actor, self).__init__()\n",
    "        self.args = args\n",
    "        self.h = self.args.hidden_size\n",
    "        self.L = self.args.actor_num_layers\n",
    "        self.activation = activations[self.args.activation_layer]\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # input layer:\n",
    "        layers.extend([\n",
    "            nn.Linear(args.state_dim, self.h),\n",
    "            self.activation,\n",
    "        ])\n",
    "        \n",
    "        # hidden layer(s):\n",
    "        for _ in range(self.L):\n",
    "            layers.extend([\n",
    "                nn.Linear(self.h, self.h),\n",
    "                LayerNorm(self.h),\n",
    "                self.activation,\n",
    "            ])\n",
    "        \n",
    "        # output layer:\n",
    "        layers.extend([\n",
    "            nn.Linear(self.h, args.action_dim),\n",
    "            nn.Tanh(),\n",
    "        ])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state: torch.tensor):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def select_action(self, state: torch.tensor):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.args.device)\n",
    "        return self.forward(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def extract_parameters(self):\n",
    "        ''' Extract the parameters of the network and flatten it into a single vector. \n",
    "        This is used for the genetic algorithm. \n",
    "    \n",
    "        Returns:\n",
    "            torch.tensor: Flattened parameters of the network.\n",
    "        '''\n",
    "        tot_size = self.count_parameters()\n",
    "        p_vec = torch.zeros(tot_size, dtype=torch.float32).to(self.args.device)\n",
    "        i = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            p_vec[i:i+sz] = param.view(-1)\n",
    "            i += sz\n",
    "        return p_vec.detach().clone()\n",
    "            \n",
    "    def inject_parameters(self, parameters):\n",
    "        ''' Inject the parameters into the network. This is used for the genetic algorithm.\n",
    "        \n",
    "        Args:\n",
    "            parameters (torch.tensor): Flattened parameters of the network.\n",
    "        '''\n",
    "        i = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            raw = parameters[i:i+sz]\n",
    "            reshaped = raw.reshape(param.shape)\n",
    "            param.data.copy_(reshaped.data)\n",
    "            i += sz\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        ''' Count the number of parameters in the network.'''\n",
    "        count = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            count += np.prod(param.shape)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([108])\n"
     ]
    }
   ],
   "source": [
    "# test actor:\n",
    "actor = Actor(Parameters(), init=True)\n",
    "\n",
    "# for name, param in actor.named_parameters():\n",
    "#     print(param.data.view(-1))\n",
    "print(actor.extract_parameters().shape)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionStrategy:\n",
    "    def __init__(self, actor: Actor, pop_size = 10):\n",
    "        super(EvolutionStrategy, self).__init__()\n",
    "        self.pop_size = pop_size\n",
    "        self.pop = [actor for _ in range(pop_size)]\n",
    "        \n",
    "    def ask(self):\n",
    "        pass\n",
    "    \n",
    "    def tell(self, fitness_lst):\n",
    "        pass\n",
    "    \n",
    "    def result(self):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
