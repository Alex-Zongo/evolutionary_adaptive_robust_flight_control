{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Strategies tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from environments.lunarlander import LunarLanderWrapper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from core_algorithms.replay_memory import ReplayMemory\n",
    "from core_algorithms.utils import calc_smoothness, Episode\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test layernorm:\n",
    "lnorm = LayerNorm(4)\n",
    "for name, param in lnorm.named_parameters():\n",
    "    print(name, param)\n",
    "    print(len(param.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self, conf={}, hidden_size=16, init=True):\n",
    "        if not init:\n",
    "            return\n",
    "        \n",
    "        use_cuda = False\n",
    "        if hasattr(conf, 'disable_cuda'):\n",
    "            use_cuda = not conf.disable_cuda and torch.cuda.is_available()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.individual_bs = 10000\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.actor_num_layers = 2\n",
    "        self.activation_layer = 'tanh'\n",
    "        \n",
    "        self.state_dim = 6\n",
    "        self.action_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'leakyRelu': nn.LeakyReLU(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, args: Parameters, init=False):\n",
    "        super(Actor, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.buffer = ReplayMemory(self.args.individual_bs)\n",
    "        self.critical_buffer = ReplayMemory(self.args.individual_bs)\n",
    "        self.h = self.args.hidden_size\n",
    "        self.L = self.args.actor_num_layers\n",
    "        self.activation = activations[self.args.activation_layer]\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # input layer:\n",
    "        layers.extend([\n",
    "            nn.Linear(args.state_dim, self.h),\n",
    "            self.activation,\n",
    "        ])\n",
    "        \n",
    "        # hidden layer(s):\n",
    "        for _ in range(self.L):\n",
    "            layers.extend([\n",
    "                nn.Linear(self.h, self.h),\n",
    "                LayerNorm(self.h),\n",
    "                self.activation,\n",
    "            ])\n",
    "        \n",
    "        # output layer:\n",
    "        layers.extend([\n",
    "            nn.Linear(self.h, args.action_dim),\n",
    "            nn.Tanh(),\n",
    "        ])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state: torch.tensor):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def select_action(self, state: torch.tensor):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.args.device)\n",
    "        return self.forward(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def extract_parameters(self):\n",
    "        ''' Extract the parameters of the network and flatten it into a single vector. \n",
    "        This is used for the genetic algorithm. \n",
    "    \n",
    "        Returns:\n",
    "            torch.tensor: Flattened parameters of the network.\n",
    "        '''\n",
    "        tot_size = self.count_parameters()\n",
    "        p_vec = torch.zeros(tot_size, dtype=torch.float32).to(self.args.device)\n",
    "        i = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            p_vec[i:i+sz] = param.view(-1)\n",
    "            i += sz\n",
    "        return p_vec.detach().clone()\n",
    "            \n",
    "    def inject_parameters(self, parameters):\n",
    "        ''' Inject the parameters into the network. This is used for the genetic algorithm.\n",
    "        \n",
    "        Args:\n",
    "            parameters (torch.tensor): Flattened parameters of the network.\n",
    "        '''\n",
    "        i = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            raw = parameters[i:i+sz]\n",
    "            reshaped = raw.reshape(param.shape)\n",
    "            param.data.copy_(reshaped.data)\n",
    "            i += sz\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        ''' Count the number of parameters in the network.'''\n",
    "        count = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            count += np.prod(param.shape)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test actor:\n",
    "actor = Actor(Parameters(), init=True)\n",
    "\n",
    "# for name, param in actor.named_parameters():\n",
    "#     print(param.data.view(-1))\n",
    "print(actor.extract_parameters().shape)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionStrategy:\n",
    "    def __init__(self, actor: Actor, pop_size = 10):\n",
    "        super(EvolutionStrategy, self).__init__()\n",
    "        self.pop_size = pop_size\n",
    "        self.pop = [actor for _ in range(pop_size)]\n",
    "        \n",
    "    def ask(self):\n",
    "        pass\n",
    "    \n",
    "    def tell(self, fitness_lst):\n",
    "        pass\n",
    "    \n",
    "    def result(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Parameters()\n",
    "params.action_dim = env.action_space.shape[0]\n",
    "params.state_dim = env.observation_space.shape[0]\n",
    "params.action_dim, params.state_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitness function - Actor evaluation\n",
    "###### reward + actor action smoothness parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent: Actor, is_action_noise: bool, store_transition: bool):\n",
    "    \"\"\" Evaluate the agent in the environment. One episode \n",
    "    is played and the total reward is returned.\n",
    "    Args:\n",
    "        agent (Agent): The agent to evaluate.\n",
    "        is_action_noise (bool): Whether to add action noise.\n",
    "        store_transition (bool): Whether to store the transition.\n",
    "    \"\"\"\n",
    "    state_lst, rewards, action_lst = [], [], []\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    done = False\n",
    "    agent.eval()\n",
    "    while not done:\n",
    "        action = agent.select_action(obs)\n",
    "        if is_action_noise:\n",
    "            action = np.clip(action + np.random.normal(0, 0.1, size=env.action_space.shape[0]), -1.0, 1.0)\n",
    "        next_obs, reward, done, truncated, _ = env.step(action.flatten())\n",
    "        action_lst.append(action.flatten())\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if store_transition:\n",
    "            # TODO: store transition to a replay buffer:\n",
    "            transition = (obs, action, next_obs, reward, float(done))\n",
    "            agent.buffer.push(*transition)\n",
    "        state_lst.append(obs)\n",
    "        obs = next_obs\n",
    "    env.close()\n",
    "    \n",
    "    actions = np.asarray(action_lst)\n",
    "    smoothness = calc_smoothness(actions, plot_spectra=False)\n",
    "    fitness = np.sum(rewards) + smoothness\n",
    "    return fitness, smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_fn(x: torch.tensor, params: Parameters):\n",
    "    \"\"\" fitness function\n",
    "    Args:\n",
    "        x: A tensor of shape (lambda, N)\n",
    "    \"\"\"\n",
    "    #TODO: Evaluate the population:\n",
    "    num_evals = 3\n",
    "    \n",
    "    # print(x.shape)\n",
    "    lamda, N = x.shape\n",
    "    smoothness_lst = []\n",
    "    fitness_lst = torch.zeros((num_evals, lamda))\n",
    "    for j in tqdm(range(lamda), total=lamda, desc='Population Eval', colour='green'):\n",
    "        for i in range(num_evals):\n",
    "            agent = Actor(params, init=True)\n",
    "            agent.inject_parameters(x[j])\n",
    "            fitness, sm = evaluate_agent(agent=agent, is_action_noise=True, store_transition=False)\n",
    "            \n",
    "            smoothness_lst.append(sm)\n",
    "            fitness_lst[i, j] = fitness\n",
    "    smoothness_lst = np.asarray(smoothness_lst)\n",
    "    pop_fitness = fitness_lst.mean(dim=0)\n",
    "    sm_avg = smoothness_lst.mean()\n",
    "    sm_sd = smoothness_lst.std()\n",
    "    return pop_fitness.reshape(-1, 1)\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(5-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMA-ES: Covariance Matrix Adaptation - Evolution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CMA(object):\n",
    "    \"\"\" \n",
    "    Covariance Matrix Adaptation Evolution Strategy (CMA-ES) implemented with pytorch\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            initial_solution: torch.tensor,\n",
    "            initial_step_size,\n",
    "            fitness_function,\n",
    "            population_size=None,\n",
    "            enforce_bounds=None,\n",
    "            cc=None,\n",
    "            c_sigma=None,\n",
    "            c_mu=None,\n",
    "            c1=None,\n",
    "            damps=None,\n",
    "            termination_no_effect=1e-8,\n",
    "            store_trace=False,\n",
    "            callback_function=None,\n",
    "            dtype=torch.float32,\n",
    "     ):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            initial_solution: search start point, numpy array\n",
    "            \n",
    "            initial_step_size: standard deviation of the covariance matrix\n",
    "            \n",
    "            fitness_function: function to be minimized\n",
    "            \n",
    "            population_size: number of samples produced at each generation\n",
    "            \n",
    "            enforce_bounds: 2D list, the min and max for each dimension. Ensures the fitness function is never called with out of bounds values.:\n",
    "            \n",
    "            cc, c_sigma, c_mu, c1, damps: hyper-parameters of CMA-ES\n",
    "           \n",
    "            termination_no_effect: termination criterion\n",
    "            \n",
    "            store_trace: store trace or not\n",
    "            \n",
    "            callback_function: callback function at the end of each generation. Intended for logging purposes.\n",
    "            \n",
    "            dtype: data type\n",
    "        \"\"\"\n",
    "        if not isinstance(initial_solution, torch.Tensor):\n",
    "            raise ValueError(\"initial_solution must be a torch.Tensor\")\n",
    "        elif initial_solution.dim() != 1:\n",
    "            ndim=initial_solution.dim()\n",
    "            raise ValueError(\"initial_solution must be a 1-D torch.Tensor, but got {}-D\".format(ndim))\n",
    "        elif not np.isscalar(initial_step_size) or initial_step_size <= 0:\n",
    "            raise ValueError(\"initial_step_size must be a positive scalar\")\n",
    "        elif not callable(fitness_function):\n",
    "            raise ValueError(\"fitness_function must be callable\")\n",
    "        elif population_size is not None and population_size <=4:\n",
    "            raise ValueError(\"population_size must be greater than 4\")\n",
    "        elif enforce_bounds is not None and not isinstance(enforce_bounds, (np.ndarray, list)):\n",
    "            raise ValueError(\"enforce_bounds must be a list or numpy array\")\n",
    "        elif enforce_bounds is not None and np.ndim(enforce_bounds) != 2:\n",
    "            raise ValueError(\"enforce_bounds must be a 2-D list or numpy array\")\n",
    "        elif callback_function is not None and not callable(callback_function):\n",
    "            raise ValueError(\"callback_function must be callable\")\n",
    "        \n",
    "        self.generation=0\n",
    "        self.initial_solution = initial_solution\n",
    "        self.initial_step_size = initial_step_size\n",
    "        self.fitness_fn = fitness_function\n",
    "        self.population_size = population_size\n",
    "        self.enforce_bounds = enforce_bounds\n",
    "        self._cc = cc\n",
    "        self._csigma = c_sigma\n",
    "        self._cmu = c_mu\n",
    "        self._c1 = c1\n",
    "        self._damps = damps\n",
    "        self.termination_no_effect = termination_no_effect\n",
    "        self.store_trace = store_trace\n",
    "        self.callback_fn = callback_function\n",
    "        self.dtype = dtype\n",
    "        self.termination_criterion_met = False\n",
    "        \n",
    "        self._initialized = False\n",
    "        \n",
    "    def init(self):\n",
    "        if self._initialized:\n",
    "            raise ValueError(\"CMA-ES has already been initialized\")\n",
    "        \n",
    "        self.generation=0\n",
    "        self.dimension = self.initial_solution.shape[0]\n",
    "        self._enforce_bounds = self.enforce_bounds is not None\n",
    "        self.trace = []\n",
    "        \n",
    "        # Solution dimension:\n",
    "        self.N = torch.tensor(self.dimension, dtype=self.dtype).int()\n",
    "        # population size:\n",
    "        if self.population_size is not None:\n",
    "            self.lamda = torch.tensor(self.population_size, dtype=self.dtype).int()\n",
    "        else:\n",
    "            self.lamda = torch.floor(3*torch.log(self.N)+8).int()\n",
    "        # shape of the population of solutions:\n",
    "        self.shape = torch.Size([self.lamda, self.N], dtype=torch.int32)\n",
    "        self.mu = torch.floor(self.lamda/2)\n",
    "        # recombination weights:\n",
    "        self.weights = torch.concat([\n",
    "        torch.log(self.mu+0.5) - torch.log(torch.arange(1, self.mu+1)),\n",
    "        torch.zeros(((self.lamda-self.mu).int(),), dtype=self.dtype),        \n",
    "    ], dim=0)\n",
    "        # normalize and reshape into column matrix:\n",
    "        self.weights = (self.weights/self.weights.sum()).reshape(-1, 1)\n",
    "        # variance effective size of mu:\n",
    "        self.mu_eff = (self.weights.sum()**2)/((self.weights**2).sum())\n",
    "        # time constant for cumulation for covariance matrix:\n",
    "        if self._cc is not None:\n",
    "            self.cc = torch.tensor(self._cc, dtype=self.dtype)\n",
    "        else:\n",
    "            self.cc = (4+self.mu_eff/self.N)/(4+self.N+2*self.mu_eff/self.N)\n",
    "        # Time constant for cumulation for step size control or sigma:\n",
    "        if self._csigma is not None:\n",
    "            self.c_sigma = torch.tensor(self._csigma, dtype=self.dtype)\n",
    "        else:\n",
    "            self.c_sigma = (self.mu_eff+2)/(self.N+self.mu_eff+5)\n",
    "        # Learning rate for rank one update of C:\n",
    "        if self._c1 is not None:\n",
    "            self.c1 = torch.tensor(self._c1, dtype=self.dtype)\n",
    "        else:\n",
    "            self.c1 = 2/((self.N+1.3)**2+self.mu_eff)\n",
    "        # Learning rate for rank mu update of C:\n",
    "        if self._cmu is not None:\n",
    "            self.c_mu = torch.tensor(self._cmu, dtype=self.dtype)\n",
    "        else:\n",
    "            self.c_mu =  2*(self.mu_eff-2+1/self.mu_eff)/((self.N+2)**2+self.mu_eff)\n",
    "        # Damping for sigma usually close to 1:\n",
    "        if self._damps is not None:\n",
    "            self.damps = torch.tensor(self._damps, dtype=self.dtype)\n",
    "        else:\n",
    "            # self.damps = 1 + self.c_sigma + 2*torch.max(torch.tensor([0, torch.sqrt((self.mu_eff-1)/(self.N+1))-1]), dtype=self.dtype)\n",
    "            self.damps = 1 + self.c_sigma + 2*torch.maximum(torch.tensor(0), torch.sqrt((self.mu_eff-1)/(self.N+1))-1)\n",
    "        # Expectation of ||N(0, I)|| == norm(randn(N,1)):\n",
    "        self.chiN = torch.sqrt(self.N)*(1-1/(4*self.N)+1/(21*self.N**2))\n",
    "        \n",
    "        # TODO: define bounds in a format to be fed in torch:\n",
    "        if self._enforce_bounds:\n",
    "            bounds = torch.tensor(self.enforce_bounds, dtype=self.dtype)\n",
    "            self.clip_value_min = bounds[:, 0]\n",
    "            self.clip_value_max = bounds[:, 1]\n",
    "            \n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # Trainable Params:\n",
    "        # ----------------------------------------\n",
    "        # Mean\n",
    "        # self.m = torch.tensor(self.initial_solution, dtype=self.dtype)\n",
    "        self.m = self.initial_solution.clone().detach()\n",
    "        # Step size or sigma:\n",
    "        self.sigma = torch.tensor(self.initial_step_size, dtype=self.dtype)\n",
    "        # Covariance matrix:\n",
    "        self.C = torch.eye(self.N, dtype=self.dtype)\n",
    "        # Evolution path for sigma:\n",
    "        self.p_sigma = torch.zeros(self.N, dtype=self.dtype)\n",
    "        # Evolution path for C:\n",
    "        self.p_C = torch.zeros(self.N, dtype=self.dtype)\n",
    "        # Coordinate system (normalized eigenvectors)\n",
    "        self.B = torch.eye(self.N, dtype=self.dtype)\n",
    "        # scaling (square root of the eigenvalues):\n",
    "        self.D = torch.eye(self.N, dtype=self.dtype)\n",
    "        \n",
    "        self._initialized = True\n",
    "        return self\n",
    "    \n",
    "    def search(self, max_generations=500):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            max_generations: maximum number of generations\n",
    "            note that the search can be interrupted by the termination criterion\n",
    "        Returns:\n",
    "            the best solution and its fitness value:\n",
    "        \"\"\"\n",
    "        if not self._initialized:\n",
    "            self.init()\n",
    "        \n",
    "        # call user defined callback function at generation 0:\n",
    "        if self.callback_fn is not None:\n",
    "            self.callback_fn(self, logger)\n",
    "            \n",
    "        for _ in range(max_generations):\n",
    "            self.generation += 1\n",
    "            print(\"Generation: \", self.generation)\n",
    "            # -----------------------\n",
    "            # (1) Sample a new population of solutions ~ N(m, sigma^2*C)\n",
    "            # -----------------------\n",
    "            z = torch.randn(self.shape, dtype=self.dtype) # sample N(0, I)\n",
    "            y = torch.matmul(z, torch.matmul(self.B, self.D))\n",
    "            x = self.m + self.sigma * y\n",
    "            print(x.shape)\n",
    "            \n",
    "            penalty = 0.0\n",
    "            if self._enforce_bounds:\n",
    "                x_corr = torch.clip(x, self.clip_value_min, self.clip_value_max)\n",
    "                penalty = torch.norm(x-x_corr)**2\n",
    "                x = x_corr\n",
    "                \n",
    "            # -----------------------\n",
    "            # (2) Selection and Recombination: Moving the mean:\n",
    "            # -----------------------\n",
    "            # Evaluate and sort solutions:\n",
    "            f_x = self.fitness_fn(x) + penalty\n",
    "            self.x_sorted = x[f_x.argsort()]\n",
    "            \n",
    "            # TODO: store trace if store_trace is True\n",
    "            if self.store_trace:\n",
    "                self._store_trace()\n",
    "            \n",
    "            # Update the mean as the weighted average of the top-mu solutions:\n",
    "            x_diff = (self.x_sorted - self.m)\n",
    "            x_mean = torch.multiply(self.weights, x_diff).sum(axis=0)\n",
    "            m = self.m + x_mean\n",
    "            \n",
    "            # --------------------\n",
    "            # (3) Adapting the Covariance Matrix:\n",
    "            # --------------------\n",
    "            # Update the evolution path for rank-one-update:\n",
    "            y_mean = x_mean / self.sigma\n",
    "            p_C = (\n",
    "                (1-self.cc)*self.p_C + \n",
    "                torch.sqrt(self.cc*(2-self.cc)*self.mu_eff) * y_mean\n",
    "            )\n",
    "            p_C_matrix = p_C[:, None]\n",
    "            \n",
    "            # Compute Rank-mu update:\n",
    "            C_m = torch.stack([torch.outer(e,e) for e in x_diff/self.sigma])\n",
    "            y_s = torch.multiply(C_m, self.weights[:,None]).sum(axis=0)\n",
    "            \n",
    "            # Combine rank-one and rank-mu update to obtain the new covariance matrix:\n",
    "            C = (\n",
    "                (1-self.c1-self.c_mu)*self.C + \n",
    "                self.c1 * p_C_matrix * p_C_matrix.T +\n",
    "                self.c_mu * y_s\n",
    "            )\n",
    "            \n",
    "            # Enforce the symmetry of the covariance matrix:\n",
    "            # C_upper = \n",
    "            # C_upper_no_diag = \n",
    "            C = torch.triu(C) + torch.triu(C, 1).T\n",
    "            \n",
    "            # ---------------------------------\n",
    "            # (4) Step size control:\n",
    "            # ---------------------------------\n",
    "            # Update the evolution path for sigma:\n",
    "            D_inv = torch.diag(torch.reciprocal(torch.diag(self.D)))\n",
    "            C_inv_squared = torch.matmul(torch.matmul(self.B, D_inv), self.B.T)\n",
    "            C_inv_squared_y = torch.squeeze(torch.matmul(C_inv_squared, y_mean[:, None]))\n",
    "            p_sigma = (\n",
    "                (1-self.c_sigma) * self.p_sigma +\n",
    "                torch.sqrt(self.c_sigma*(2-self.c_sigma)*self.mu_eff) * C_inv_squared_y\n",
    "            )\n",
    "            \n",
    "            # update sigma:\n",
    "            sigma = self.sigma * torch.exp((self.c_sigma/self.damps)*(torch.norm(p_sigma)/self.chiN - 1))\n",
    "            \n",
    "            # -----------------------------------------\n",
    "            # (5) Update B and D: eigen decomposition:\n",
    "            # -----------------------------------------\n",
    "            B, D_squared, _ = torch.svd(C)\n",
    "            diag_D = torch.sqrt(D_squared)\n",
    "            D = torch.diag(diag_D)\n",
    "            \n",
    "            # -----------------------------------------\n",
    "            # (6) Assign new variable values:\n",
    "            # -----------------------------------------\n",
    "            # Cache computations necessary for the termination criterion:\n",
    "            self._prev_sigma = self.sigma\n",
    "            self._prev_D = self.D\n",
    "            self._diag_D = diag_D\n",
    "            \n",
    "            # Assign values:\n",
    "            self.p_C = p_C\n",
    "            self.p_sigma = p_sigma\n",
    "            self.C = C\n",
    "            self.sigma = sigma\n",
    "            self.B = B\n",
    "            self.D = D\n",
    "            self.m = m\n",
    "            \n",
    "            # ------------------------------------------\n",
    "            # (7) Terminate early if necessary:\n",
    "            # ------------------------------------------\n",
    "            self.termination_criterion_met = self.should_terminate()\n",
    "            \n",
    "            # Call user defined function last:\n",
    "            if self.callback_fn is not None:\n",
    "                self.callback_fn(self, logger)\n",
    "            \n",
    "            if self.termination_criterion_met:\n",
    "                break\n",
    "        return self.best_solution(), self.best_fitness()\n",
    "    \n",
    "    def inject_RL(self, rl_actor):\n",
    "        pass\n",
    "    \n",
    "    def best_solution(self):\n",
    "        return self.m.detach().numpy()\n",
    "    \n",
    "    def best_fitness(self):\n",
    "        return self.fitness_fn(self.m).detach().numpy()\n",
    "    \n",
    "    def should_terminate(self, returns_details=False):\n",
    "        i = self.generation % self.dimension\n",
    "        # NoEffectAxis: stop if adding 0.1 stdev in any single principal axis in the direction of C does not change the solution:  \n",
    "        m_nea = self.m + 0.1 * self.sigma * torch.squeeze(self._diag_D[i] * self.B[i, :])\n",
    "        m_nea_diff = (self.m - m_nea).abs()\n",
    "        no_effect_axis = torch.all(m_nea_diff < self.termination_no_effect)\n",
    "        \n",
    "        # NoEffectCoord: stop if adding 0.2 stdev in any single coordinate does not change the solution:\n",
    "        m_nec = self.m + 0.2 * self.sigma * torch.diag(self.C)\n",
    "        m_nec_diff = (self.m - m_nec).abs()\n",
    "        no_effect_coord = torch.any(m_nec_diff < self.termination_no_effect)\n",
    "        \n",
    "        # ConditionCov: stop if the condition number of the covariance matrix exceeds 10^14:\n",
    "        max_D = torch.max(self._diag_D)\n",
    "        min_D = torch.min(self._diag_D)\n",
    "        condition_number = max_D**2 / min_D**2\n",
    "        condition_cov = condition_number > 1e14\n",
    "        \n",
    "        # TolXUup: stop if sigma * max(D) increased by more than 10^4:\n",
    "        # this usually indicates a far too small initial sigma or divergent behavior.\n",
    "        prev_max_D = torch.max(torch.diag(self._prev_D))\n",
    "        tol_x_up_diff = (self.sigma * max_D - self._prev_sigma * prev_max_D).abs()\n",
    "        tol_x_up = tol_x_up_diff > 1e4\n",
    "        \n",
    "        do_terminate = no_effect_axis or no_effect_coord or condition_cov or tol_x_up\n",
    "        \n",
    "        if not returns_details:\n",
    "            return do_terminate\n",
    "        else:\n",
    "            return (\n",
    "                do_terminate,\n",
    "                dict(\n",
    "                    no_effect_axis=bool(no_effect_axis.numpy()),\n",
    "                    no_effect_coord=bool(no_effect_coord.numpy()),\n",
    "                    condition_cov=bool(condition_cov.numpy()),\n",
    "                    tol_x_up=bool(tol_x_up.numpy()),\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    def reset(self):\n",
    "        self._initialized = False\n",
    "        return self.init()\n",
    "    \n",
    "    def _store_trace(self):\n",
    "        self.trace.append({\n",
    "            'm': self.m.detach().numpy(),\n",
    "            'sigma': self.sigma.detach().numpy(),\n",
    "            'C': self.C.detach().numpy(),\n",
    "            'p_sigma': self.p_sigma.detach().numpy(),\n",
    "            'p_C': self.p_C.detach().numpy(),\n",
    "            'B': self.B.detach().numpy(),\n",
    "            'D': self.D.detach().numpy(),\n",
    "            'population': self.x_sorted.detach().numpy(),\n",
    "        })\n",
    "        \n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(4).int()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CMA-ES on Lunar Lander Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_fn(cma: CMA, logger):\n",
    "    if cma.generation % 10 == 0:\n",
    "        logger.info(f'Generation {cma.generation}: {cma.best_fitness()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sln = Actor(args=params, init=True).extract_parameters()\n",
    "type(sln), sln.dim()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "initial_solution = torch.randn(sln_dim)\n",
    "initial_step_size = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_solution.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cma = CMA(\n",
    "    initial_solution=initial_solution,\n",
    "    initial_step_size=initial_step_size,\n",
    "    fitness_function=fitness_fn,\n",
    "    # callback_function=callback_fn,\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cma.init()\n",
    "cma.lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cma.search(max_generations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_weight_decay(weight_decay, model_params):\n",
    "    model_params_grid = torch.tensor(model_params)\n",
    "    return -weight_decay * torch.mean(model_params_grid * model_params_grid, dim=1)\n",
    "\n",
    "\n",
    "class CMAES:\n",
    "    '''CMA-ES algorithm implementation (Wrapper)'''\n",
    "\n",
    "    def __init__(self, num_params, sigma_init=0.10, pop_size=20, weight_decay=0.01):\n",
    "        self.num_params = num_params\n",
    "        self.sigma_init = sigma_init\n",
    "        self.pop_size = pop_size\n",
    "        self.weight_decay = weight_decay\n",
    "        self.solutions = None\n",
    "\n",
    "        import cma\n",
    "        self.es = cma.CMAEvolutionStrategy(\n",
    "            self.num_params * [0], self.sigma_init, {'popsize': self.pop_size})\n",
    "\n",
    "    def rms_stdev(self):\n",
    "        sigma = self.es.result[6]\n",
    "        return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "    def ask(self):\n",
    "        '''Returns a list of solutions'''\n",
    "        self.solutions = np.array(self.es.ask())\n",
    "        return torch.tensor(self.solutions)\n",
    "\n",
    "    def tell(self, fitness_list):\n",
    "        fitness_table = torch.tensor(np.array(fitness_list))\n",
    "        if self.weight_decay > 0:\n",
    "            l2_decay = compute_weight_decay(self.weight_decay, self.solutions).reshape(-1,1)\n",
    "            print(l2_decay.shape)\n",
    "            fitness_table += l2_decay\n",
    "        # convert minimizer to maximizer:\n",
    "        self.es.tell(self.solutions,\n",
    "                     (-fitness_table).tolist())\n",
    "\n",
    "    def current_param(self):\n",
    "        return self.es.result[5]  # mean solution, presumably better with noise\n",
    "\n",
    "    def set_mu(self):\n",
    "        pass\n",
    "\n",
    "    def best_param(self):\n",
    "        return self.es.result[0]  # best evaluated solution\n",
    "\n",
    "    def result(self):\n",
    "        '''return the best params so far along with historically best reward, current reward, sigma'''\n",
    "        r = self.es.result\n",
    "        return (r[0], -r[1], -r[1], r[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_algorithms.cmaes import CMAES\n",
    "from core_algorithms.cmaes import fitness_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Parameters(hidden_size=64)\n",
    "params.action_dim = env.action_space.shape[0]\n",
    "params.state_dim = env.observation_space.shape[0]\n",
    "num_params = Actor(args=params, init=True).count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = torch.floor(3*torch.log(torch.tensor(num_params))+8).int()\n",
    "num_params, lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = CMAES(num_params, pop_size=10, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sln = strategy.ask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sln.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = fitness_fn(sample_sln, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy.tell(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy.best_param().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sltn = Actor(args=params, init=True)\n",
    "sltn.inject_parameters(torch.tensor(strategy.best_param()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sltn.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_fit = []\n",
    "max_gen = 200\n",
    "generation = 0\n",
    "for _ in range(max_gen):\n",
    "    generation = generation + 1\n",
    "    print(\"Generation: \", generation)\n",
    "    solution = strategy.ask()\n",
    "    fx = fitness_fn(solution, params)\n",
    "    max_fit = fx.max().detach().numpy()\n",
    "    pop_fit.append(max_fit)\n",
    "    print(\"maximum fitness: \", max_fit)\n",
    "    strategy.tell(fx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pop_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn((3,3))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = torch.svd(c)\n",
    "u, s, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u @ u.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag(torch.reciprocal(torch.diag(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_up = torch.triu(c)\n",
    "torch.triu(c) + torch.triu(c, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_diff = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "C_m = torch.stack([torch.outer(e, e) for e in x_diff/2])\n",
    "\n",
    "C_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.reshape(-1, 1)\n",
    "w[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiply(C_m, w[:, None]).sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = torch.tensor(24)\n",
    "mu=torch.floo\n",
    "\n",
    "weights = torch.concat([\n",
    "    torch.log(mu+0.5) - torch.log(torch.arange(1, mu+1)),\n",
    "    torch.zeros(size=(lamda-mu,), dtype=torch.float32),\n",
    "], dim=0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.config import select_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_env = select_env(\"Phlab_attitude_nominal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs =ph_env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_bar = obs + np.random.normal(0, 0.05, size=obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0.1 * F.mse_loss(torch.tensor(obs), torch.tensor(obs_bar))\n",
    "l.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights/weights.sum()\n",
    "w.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[4, 5, 6], [1, 2, 3], [7, 8, 9]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x - torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.1, 0.2, 0.3]).reshape(-1, 1)\n",
    "torch.multiply(weights, x).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_fn(x):\n",
    "    return torch.sum(x**2, dim=1)\n",
    "\n",
    "# x = torch.randn((10, 4))\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_x = fit_fn(x)\n",
    "f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = f_x.argsort(descending=True)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[idx], x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.gather(x[:], 1, torch.argsort(f_x))\n",
    "x.gather(0, torch.argsort(f_x, 0).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters_es import ESParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLNN(nn.Module):\n",
    "    '''Base Class for all RL Neural Networks.'''\n",
    "\n",
    "    def __init__(self, args: ESParameters):\n",
    "        super(RLNN, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, state: torch.tensor):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.args.device)\n",
    "        return self.forward(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def extract_parameters(self):\n",
    "        ''' Extract the parameters of the network and flatten it into a single vector.\n",
    "        This is used for the genetic algorithm.\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: Flattened parameters of the network.\n",
    "        '''\n",
    "        tot_size = self.count_parameters()\n",
    "        p_vec = torch.zeros(tot_size, dtype=torch.float32).to(self.args.device)\n",
    "        i = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            p_vec[i:i+sz] = param.view(-1)\n",
    "            i += sz\n",
    "        return p_vec.detach().clone()\n",
    "\n",
    "    def inject_parameters(self, parameters):\n",
    "        ''' Inject the parameters into the network. This is used for the genetic algorithm.\n",
    "\n",
    "        Args:\n",
    "            parameters (torch.tensor): Flattened parameters of the network.\n",
    "        '''\n",
    "        i = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            raw = parameters[i:i+sz]\n",
    "            reshaped = raw.reshape(param.shape)\n",
    "            param.data.copy_(reshaped.data)\n",
    "            i += sz\n",
    "\n",
    "    def count_parameters(self):\n",
    "        ''' Count the number of parameters in the network.'''\n",
    "        count = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lnorm' in name or len(param.shape) != 2:\n",
    "                continue\n",
    "            count += np.prod(param.shape)\n",
    "        return count\n",
    "\n",
    "    def get_grads(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Actor(RLNN):\n",
    "    def __init__(self, params: ESParameters, rnn_type='LSTM'):\n",
    "        super(RNN_Actor, self).__init__(params)\n",
    "        self.args = params\n",
    "        self.h = self.args.actor_hidden_size\n",
    "        self.L = self.args.actor_num_layers\n",
    "        activation = activations[self.args.activation_actor.lower()]\n",
    "        self.critical_buffer = ReplayMemory(self.args)\n",
    "        self.rnn = None\n",
    "        in_layer = []\n",
    "        # input layer:\n",
    "        in_layer.extend([\n",
    "            nn.Linear(self.args.state_dim, self.h),\n",
    "            LayerNorm(self.h),\n",
    "            activation,\n",
    "        ])\n",
    "\n",
    "        # hidden RNN layers:\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(self.h, self.h, num_layers=self.L)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(self.h, self.h, num_layers=self.L)\n",
    "\n",
    "        # output layer:\n",
    "        out_layer = []\n",
    "        out_layer.extend([\n",
    "            nn.Linear(self.h, self.args.action_dim),\n",
    "            nn.Tanh(),\n",
    "        ])\n",
    "        self.in_net = nn.Sequential(*in_layer)\n",
    "        self.ou_net = nn.Sequential(*out_layer)\n",
    "        self.to(self.args.device)\n",
    "    def forward(self, state: torch.tensor):\n",
    "        h = self.in_net(state)\n",
    "        out, _ = self.rnn(h)\n",
    "        return self.ou_net(out)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ESParameters()\n",
    "params.action_dim=3\n",
    "params.state_dim=6\n",
    "params.device='cpu'\n",
    "params.actor_hidden_size=40\n",
    "params.actor_num_layers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lstm = RNN_Actor(params, rnn_type='LSTM')\n",
    "actor_rnn = RNN_Actor(params, rnn_type='RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lstm.count_parameters(), actor_rnn.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_rnn.select_action(obs), actor_lstm.select_action(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.config import select_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = select_env(\"Phlab_attitude_nominal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_algorithms.actor_model import Actor\n",
    "from parameters_es import ESParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "conf = {\n",
    "    'use_td3': True,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "params = ESParameters(conf=conf, init=True)\n",
    "params.state_dim = 6\n",
    "params.action_dim = 3\n",
    "params.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.actor_hidden_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(args=params, init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (activation): Tanh()\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=40, bias=True)\n",
      "    (1): LayerNorm()\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (4): LayerNorm()\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (7): LayerNorm()\n",
      "    (8): Tanh()\n",
      "    (9): Linear(in_features=40, out_features=3, bias=True)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3560"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_algorithms.actor_model import CriticTD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = CriticTD3(args=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d = dict(\n",
    "    actor_hidden_size=64,\n",
    "    actor_num_layers=3,\n",
    "    critic_hidden_size=[64, 128],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_actor': 'tanh',\n",
      " 'activation_critic': 'tanh',\n",
      " 'actor_hidden_size': 64,\n",
      " 'actor_lr': 0.001,\n",
      " 'actor_num_layers': 3,\n",
      " 'batch_size': 100,\n",
      " 'critic_hidden_size': [32, 64],\n",
      " 'critic_lr': 0.001,\n",
      " 'gamma': 0.99,\n",
      " 'mem_size': 1000000,\n",
      " 'n_evals': 2,\n",
      " 'n_generations': 100,\n",
      " 'noise_sd': 0.33,\n",
      " 'pop_size': 10,\n",
      " 'save_foldername': './logs',\n",
      " 'seed': 7,\n",
      " 'use_caps': False}\n"
     ]
    }
   ],
   "source": [
    "params.stdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_actor': 'tanh',\n",
      " 'activation_critic': 'tanh',\n",
      " 'actor_hidden_size': 64,\n",
      " 'actor_lr': 0.001,\n",
      " 'actor_num_layers': 3,\n",
      " 'batch_size': 100,\n",
      " 'critic_hidden_size': [64, 128],\n",
      " 'critic_lr': 0.001,\n",
      " 'gamma': 0.99,\n",
      " 'mem_size': 1000000,\n",
      " 'n_evals': 2,\n",
      " 'n_generations': 100,\n",
      " 'noise_sd': 0.33,\n",
      " 'pop_size': 10,\n",
      " 'save_foldername': './logs',\n",
      " 'seed': 7,\n",
      " 'use_caps': False}\n"
     ]
    }
   ],
   "source": [
    "params.update_from_dict(new_d)\n",
    "params.stdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17453292519943295"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.deg2rad(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
